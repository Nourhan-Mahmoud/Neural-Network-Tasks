{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 495,
   "id": "3acf4b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm \n",
    "import seaborn as sns\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56247d9",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "c0e24b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fdata = pd.DataFrame(pd.read_csv(\"mnist_train.csv\"))\n",
    "test_fdata  = pd.DataFrame(pd.read_csv(\"mnist_test.csv\"))\n",
    "train_label = np.array(train_fdata[\"label\"])\n",
    "train_data= np.array(train_fdata.drop(\"label\" , axis =1))\n",
    "test_label=np.array(test_fdata['label'])\n",
    "test_data=np.array(test_fdata.drop(\"label\",  axis =1 ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ce547e",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "f40fa5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_labels(data):\n",
    "    train_label_encoded=np.zeros((10,data.shape[0]))\n",
    "    for ind in range (data.shape[0]):\n",
    "        val=data[ind]\n",
    "        for row in range (10):\n",
    "            if (val==row):\n",
    "                train_label_encoded[val,ind]=1\n",
    "    return train_label_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "bc8b6cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 60000) (10, 60000) (784, 10000) (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "train_data =np.transpose(train_data)\n",
    "train_label=encoding_labels(train_label)\n",
    "test_data =np.transpose(test_data) \n",
    "test_label=encoding_labels(test_label)\n",
    "print(train_data.shape ,train_label.shape , test_data.shape ,test_label.shape )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8724e15",
   "metadata": {},
   "source": [
    "# Activation functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "9b4f0ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1+np.exp(-Z))\n",
    "def tanH(Z):\n",
    "    return (np.tanh(Z)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adffac6d",
   "metadata": {},
   "source": [
    "# Derivative of activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "866a5809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(Z):    \n",
    "    s = sigmoid(Z)\n",
    "    dZ = s * (1-s)\n",
    "    return dZ\n",
    "def tanH_backward(Z):\n",
    "    tanh = tanH(Z)\n",
    "    dZ = (1 + tanh)*( 1- tanh) \n",
    "    return dZ\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c57ace",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "efe77344",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(num_of_hidden_layers, number_of_neurons ,num_of_epochs  ,input_layer, output_layer ,\n",
    "                          Activation_func = \"sigmoid\" , Add_bias=True , eta = 0.01  ):\n",
    "    #initialize parameters\n",
    "    parameters = {}\n",
    "    parameters[\"Activation_function\"]=Activation_func\n",
    "    parameters[\"learning_rate\"] =eta\n",
    "    parameters[\"Num_of_epochs\"] = num_of_epochs\n",
    "    parameters[\"Add_bias\"] = Add_bias\n",
    "    parameters[\"number_of_layers\"] =num_of_hidden_layers+1\n",
    "    \n",
    "    #initialize weights\n",
    "    weights ={}\n",
    "    L = num_of_hidden_layers+2           # number of layers in the network\n",
    "    layers = [input_layer]\n",
    "    for i in range(num_of_hidden_layers):\n",
    "        layers.append(number_of_neurons)\n",
    "    layers.append(output_layer)\n",
    "    for l in range(1, L):\n",
    "        weights['W' + str(l)] = np.random.randn(layers[l] , layers[l-1])\n",
    "        weights['b' + str(l)] = np.zeros(( layers[l] ,1))\n",
    "    \n",
    "    return parameters , weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1818661b",
   "metadata": {},
   "source": [
    "# Backpropogation Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c1627c",
   "metadata": {},
   "source": [
    "<h4> Feedforward propagation</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "a609a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feedforward_propagation(X, parameters , weights):\n",
    "    caches = []\n",
    "    A = X\n",
    "    num_of_layers = parameters[\"number_of_layers\"]+1             \n",
    "    for l in range(1, num_of_layers):\n",
    "        W ,b , activation = weights['W' + str(l)], weights['b' + str(l)],parameters[\"Activation_function\"]\n",
    "        Z = (np.dot(W,A)) +b\n",
    "        cache = (A , W , b , Z)\n",
    "        if activation == \"sigmoid\":\n",
    "            A = sigmoid(Z)\n",
    "        elif activation == \"tanH\":\n",
    "            A = tanH(Z)\n",
    "        caches.append(cache)               \n",
    "    return A , caches "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb2ebb6",
   "metadata": {},
   "source": [
    "<h4> Backward Model</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "67c6bb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_backward(dA, cache, activation):\n",
    "    A_prev, W, b, Z = cache\n",
    "    if activation == \"tanH\":\n",
    "        dZ = dA* tanH_backward(Z) \n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = dA* sigmoid_backward(Z)\n",
    "    m = A_prev.shape[1]\n",
    "    dW = 1./m * np.dot(dZ,A_prev.T)  \n",
    "    db = (1/m)*np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "id": "87559f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches  , parameters ):\n",
    "    #output layer update\n",
    "    grads = {}\n",
    "    L = parameters[\"number_of_layers\"]  \n",
    "    E = AL - Y\n",
    "    current_cache = caches[L-1]\n",
    "    dA_prev_temp, dW_temp, db_temp  = activation_backward(E, current_cache,parameters[\"Activation_function\"])\n",
    "    grads[\"dA\"+str(L)]=dA_prev_temp\n",
    "    grads[\"dW\"+str(L)]=dW_temp\n",
    "    grads[\"db\"+str(L)]=db_temp\n",
    "    # hidden layersupdate\n",
    "    for l in reversed(range(L-1)):\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = activation_backward(grads[\"dA\" + str(l +2)], current_cache,parameters[\"Activation_function\"])\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)]=db_temp\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377f40b1",
   "metadata": {},
   "source": [
    "<h4> update parameters</h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "id": "9a1629a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(weights, grads, learning_rate ,parameters ):\n",
    "    for l in range(parameters[\"number_of_layers\"]):\n",
    "        weights[\"W\" + str(l+1)] =weights[\"W\" + str(l+1)] + (learning_rate*grads[\"dW\" + str(l+1)])\n",
    "        if(parameters[\"Add_bias\"]):\n",
    "            weights[\"b\" + str(l+1)] = weights[\"b\" + str(l+1)] + (learning_rate*grads[\"db\" + str(l+1)])\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "id": "88e4a783",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(parameters , weights , X ,Y):\n",
    "    for i in range(0, parameters[\"Num_of_epochs\"]):\n",
    "        # Implementing feedforward propagation\n",
    "        A1 , caches = feedforward_propagation(X, parameters , weights)\n",
    "        # Calculating error \n",
    "        L = len(caches)\n",
    "        grads = L_model_backward(A1, Y, caches ,parameters)\n",
    "        #updating parameters\n",
    "        weights = update_parameters(weights, grads, parameters[\"learning_rate\"] ,parameters) \n",
    "    return parameters , weights   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "id": "190f5a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit(X, Y, learning_rate , num_iterations ,num_of_hidden_layers , number_of_neurons , Activation_func, Add_bias=True):\n",
    "    parameters ,weights = initialize_parameters(num_of_hidden_layers, number_of_neurons ,num_iterations , X.shape[0],\n",
    "    Y.shape[0] , Activation_func , Add_bias=True , eta = learning_rate)\n",
    "    parameters ,weights  = model(parameters , weights , X ,Y)\n",
    "    return parameters ,weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "id": "2a43fc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-499-e2efeb81a8db>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-Z))\n"
     ]
    }
   ],
   "source": [
    "parameters,weights = fit(train_data, train_label, 0.05 ,  5 , 4 , 16 , \"sigmoid\" ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "id": "c5780740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(A, Y):\n",
    "    m = Y.shape[1]\n",
    "    count =0\n",
    "    for i in range(m):\n",
    "        ind1 = -1\n",
    "        for j in range(10):\n",
    "            if Y[j][i]==1:\n",
    "                ind1=j\n",
    "                break\n",
    "        ind2=-1\n",
    "        maxi= -400\n",
    "        for j in range(10):\n",
    "            if A[j][i]>maxi:\n",
    "                ind2=j\n",
    "                maxi=A[j][i]\n",
    "       # print(ind1 ,ind2,A[ind2][i] )\n",
    "        if ind1 == ind2:\n",
    "            count+=1      \n",
    "    return count/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "a540c2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-499-e2efeb81a8db>:2: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/(1+np.exp(-Z))\n"
     ]
    }
   ],
   "source": [
    "a ,_= feedforward_propagation(test_data, parameters,weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "id": "24d67fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0958"
      ]
     },
     "execution_count": 511,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_cost(a ,test_label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "073ecf7711b33896c73045026cab43a57d9f4e827e131af6a8fe927206597a43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
